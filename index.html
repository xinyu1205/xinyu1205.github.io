<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xinyu Huang (ÈªÑÊñ∞ÂÆá)</title>
  
  <meta name="author" content="Xinyu Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üê†</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <p style="text-align:center">
                  <name>Xinyu Huang (ÈªÑÊñ∞ÂÆá)</name>
                </p>
                <p>
                  I am a final-year Ph.D. student at the <a href="https://cs.fudan.edu.cn/">School of Computer Science, Fudan University</a>, advised by 
                  Prof. <a href="https://faculty.fudan.edu.cn/fengrui/zh_CN/index.htm">Rui Feng</a> and Prof. Yuejie Zhang. Currently, I'm visiting the <a href="https://www.mmlab-ntu.com/">MMLab</a> at <a href="https://www.ntu.edu.sg/">Nanyang Technological University</a> under the supervision of Prof. <a href="https://liuziwei7.github.io/">Ziwei Liu</a> from March to September 2025.
                </p>
                <p>
                  From 2020 to 2024, I researched on visual perception and created the <a href="https://github.com/xinyu1205/recognize-anything" style="color: red;">Recognize Anything Model (RAM) Family</a>: a series of open-source and powerful image perception models that exceed CLIP by more than 20 points in fine-grained perception. This work was done at OPPO Research Institute & IDEA, where I was fortunate to collaborate with <a href="https://scholar.google.com/citations?user=j5idDP8AAAAJ&hl=zh-CN&oi=ao">Youcai Zhang</a>, Prof. <a href="https://scholar.google.com/citations?user=fWDoWsQAAAAJ&hl=zh-CN&oi=ao">Yandong Guo</a>, and Prof. <a href="https://www.leizhang.org/">Lei Zhang</a>.
                </p>
                <p>
                  From 2024 to 2025, I co-leader the full process construction (Pretrain, SFT, RL) of large multimodal models at TikTok AI Innovation Center, which achieve or exceed Qwen2.5-VL on 30+ benchmarks. During this period, I was fortunate to work closely with <a href="https://brianboli.com/">Bo Li</a>, <a href="https://scholar.google.com/citations?user=q8ZrKVIAAAAJ&hl=zh-CN&oi=ao">Wei Li</a>, and <a href="https://scholar.google.com/citations?hl=zh-CN&user=XwY9LXoAAAAJ">Zejun Ma</a>.
                </p>
                <p>
                  <!-- <strong>I expect to graduate in September 2025. I am open to both academic and industry research positions. Please feel free to download my <a href="images/resume_xinyuhuang.pdf">Resume</a> and do not hesitate to email me if you're interested :)</strong> -->
                </p>
              <p style="text-align:center">
<!--                 <a href="images/resume_xinyuhuang.pdf">Resume</a> &nbsp/&nbsp -->
                <a href="mailto:xinyuhuang20@fudan.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=1O5b3VcAAAAJ&hl=zh-CN&oi=ao">Scholar</a>
                &nbsp/&nbsp
                <a href="https://github.com/xinyu1205">Github</a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/xin-yu-77-88-42">Zhihu</a>
              </p>
            <!-- </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/xinyuhuang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/xinyuhuang.jpg" class="hoverZoomLink"></a>
            </td> -->
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/huangxinyu.jpg"><img style="width:80%;max-width:80" alt="profile photo"
                  src="images/huangxinyu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <!-- <heading>Research</heading>(* indicates equal contribution) -->
                <heading>Research</heading> (* indicates equal contribution)

          </td>
            </tr>
        </tbody></table>
        <style>
          img {
            border-radius: 15px;
          }
        </style>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ram_plus_compare.jpg" alt="tag2text" width="190" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2310.15200">
                <papertitle> Recognize Anything Plus Model (RAM++) </papertitle>
              </a>
              <papertitle> <br>Open-Set Image Tagging with Multi-Grained Text Supervision </papertitle>
              <br>
              <strong>Xinyu Huang</strong>, Yi-Jie Huang, Youcai Zhang, Weiwei Tian, Rui Feng, Yuejie Zhang, Yanchun Xie, Yaqian Li, Lei Zhang
              <br>
              <em>Arxiv</em>,
              2023
              <br>
              <!-- <a href="https://recognize-anything.github.io/">project page</a> -->
              
              <a href="https://arxiv.org/abs/2306.03514">arXiv</a>
              /
              <!-- <a href="https://huggingface.co/spaces/xinyu1205/Tag2Text">demo</a> -->
              
              <a href="https://github.com/xinyu1205/Recognize_Anything-Tag2Text">code</a>
              <p></p>
              <p>RAM++ is the next generation of RAM, which can <strong>recognize any category with high accuracy</strong>, including <strong>both predefined common categories and diverse open-set categories</strong>.    </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/localization_and_recognition.jpg" alt="tag2text" width="190" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://recognize-anything.github.io/">
                <papertitle> Recognize Anything Model (RAM) </papertitle>
              </a>
              <papertitle> <br>Recognize Anything: A Strong Image Tagging Model </papertitle>
              <br>
              Youcai Zhang*,
              <strong>Xinyu Huang*</strong>,
              Jinyu Ma*, Zhaoyang Li*, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, Yandong Guo, Lei Zhang
              <br>
              <strong><em>CVPR 2024, Multimodal Foundation Models Workshop</em></strong>
              <br>
              <a href="https://recognize-anything.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2306.03514">arXiv</a>
              /
              <a href="https://huggingface.co/spaces/xinyu1205/Tag2Text">demo</a>
              /
              <a href="https://github.com/xinyu1205/Recognize_Anything-Tag2Text">code</a>
              <p></p>
              <p>RAM is an image tagging model, which can <strong>recognize any common category with high accuracy</strong>.    </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tag2text.png" alt="tag2text" width="190" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://tag2text.github.io/">
                <papertitle> Tag2Text Vision-Language Model </papertitle>
              </a>
              <papertitle> <br>Tag2Text: Guiding Vision-Language Model via Image Tagging </papertitle>
              <br>
              <strong>Xinyu Huang</strong>,
              Youcai Zhang,
              Jinyu Ma,
              Weiwei Tian,
              Rui Feng,
              Yuejie Zhang,
              Yaqian Li,
              Yandong Guo,
              Lei Zhang
              <br>
              <strong><em>ICLR 2024</em></strong>
              <br>
              <a href="https://tag2text.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2303.05657">arXiv</a>
              /
              <a href="https://huggingface.co/spaces/xinyu1205/Tag2Text">demo</a>
              /
              <a href="https://github.com/xinyu1205/Tag2Text">code</a>
              <p></p>
              <p>Tag2Text is a vision-language model guided by tagging, which can <strong>support tagging and comprehensive captioning simultaneously</strong>.              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/idea.png" alt="idea" width="190" height="140">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://arxiv.org/abs/2207.05333"> -->
                <papertitle> IDEA: Increasing Text Diversity via Online Multi-Label Recognition for Vision-Language Pre-training </papertitle>
              <!-- </a> -->
              <br>
              <strong>Xinyu Huang</strong>,
              Youcai Zhang,
              Ying Cheng,
              Weiwei Tian,
              Ruiwei Zhao,
              Rui Feng,
              Yuejie Zhang,
              Yaqian Li,
              Yandong Guo,
              Xiaobo Zhang
              <br>
              <em>ACM MM</em>,
              2022
              <br>
              <a href="https://arxiv.org/abs/2207.05333">arXiv</a>
              /
              <a href="https://github.com/xinyu1205/IDEA-pytorch">code</a>
              <p>
                We propose IDEA to provide more explicit textual supervision (including multiple valuable tags and texts composed by multiple tags) for visual models.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mlml.png" alt="mlml" width="190" height="100">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://arxiv.org/abs/2112.07368"> -->
                <papertitle>Simple and Robust Loss Design for Multi-Label Learning with Missing Labels</papertitle>
              <!-- </a> -->
              <br>
              Youcai Zhang*,
              Yuhao Cheng*,
              <strong>Xinyu Huang*</strong>,
              Fei Wen,
              Rui Feng,
              Yaqian Li,
              Yandong Guo
              <br>
              <em>Arxiv</em>,
              2021
              <br>
              <a href="https://arxiv.org/abs/2112.07368">arXiv</a>
              /
              <a href="https://github.com/xinyu1205/robust-loss-mlml">code</a>
              <p>
                Multi-label learning in the presence of missing labels(MLML) is a challenging problem. We propose two simple yet effective methods via robust loss design based on an observation. 
              </p>
            </td>
          </tr>


    </tbody></table>


    <table
    style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Projects & Resources</heading>
        </td>
      </tr>
    </tbody>
  </table>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/tagging_results.jpg" alt="tag2text" width="190" height="110">
      </td>
      <td width="75%" valign="middle">
        <a href="https://github.com/xinyu1205/recognize-anything">
          <papertitle> Recognize Anything Family</papertitle>
        </a>
        <br>
        <strong>Project Creator/Owner</strong>
        <br>
        <span style="color: red;">3.2K+ stars! </span>
        <p></p>
        <p>We provide <strong>Recognize Anything Model Family (RAM)</strong> demonstrating <strong>superior image recognition ability!</strong> </p>
      </td>
    </tr>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/tag2text_grounded_cam.png" alt="tag2text" width="190" height="110">
      </td>
      <td width="75%" valign="middle">
        <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">
          <papertitle> RAM-Grounded-SAM </papertitle>
        </a>
        <br>
        <strong>Project Co-Leader</strong>
        <br>
        <span style="color: red;">16.3K+ stars! </span>
        <p></p>
        <p><strong>RAM Faimly marry Grounded-SAM</strong>, which can <strong>automatically recognize, detect, and segment</strong> for an image! RAM Family showcases <strong>powerful image recognition capabilities!</strong> </p>
      </td>
    </tr>




</tbody></table>


			
      </td>
    </tr>
  </table>
</body>

</html>
